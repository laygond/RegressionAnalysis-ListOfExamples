{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification via a Neural Network\n",
    "\n",
    "In the the [MNIST SVM example](../svm/mnist_svm.ipynb), we introduced the classic MNIST digit classification problem and trained a simple SVM classifier for the model.  In this demo, we will try a simple neural network.  The network we will create will not perform quite as well -- we will obtain an accuracy of only around 97%, while the SVM classifier obtains an accuracy of over 98%.  However, once we understand these simple neural networks, we will be able to build more sophisticated networks that can obtain much better classification rate.  Also, in doing this demo, you will learn several important features of the `keras` package in addition to the concepts shown in the [simple neural network example](./synthetic.ipynb):\n",
    "\n",
    "* How to construct multi-class classifiers using categorical cross entropy.\n",
    "* How to optimize using mini-batches.\n",
    "* How to save and load the model after training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Keras package and the MNIST data\n",
    "\n",
    "We first load the `keras` package as in the [simple neural network example](./synthetic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load some other common packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the MNIST data as in the [MNIST SVM example](../svm/mnist_svm.ipynb).  We rescale the input `X` from values from -1 to 1 as this works better for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "\n",
    "X = 2*(mnist.data/255-0.5)   # Change X from -1 to 1\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the data into traing and test.  A standard split uses 50,000 examples for training and 10,000 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ntr = 50000\n",
    "nts = 10000\n",
    "\n",
    "Xtr, Xts, ytr, yts = train_test_split(X,y,train_size=ntr, test_size=nts,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the `plt_digit` function from the [MNIST SVM example](../svm/mnist_svm.ipynb) to display digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABeCAYAAAAUjW5fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fbb15a10f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plt_digit(x):\n",
    "    nrow = 28\n",
    "    ncol = 28\n",
    "    xsq = x.reshape((nrow,ncol))\n",
    "    plt.imshow(xsq,  cmap='Greys_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# Select random digits\n",
    "nplt = 4\n",
    "nsamp = X.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "\n",
    "# Plot the images using the subplot command\n",
    "for i in range(nplt):\n",
    "    ind = Iperm[i]\n",
    "    plt.subplot(1,nplt,i+1)\n",
    "    plt_digit(X[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the neural network, we first import the appropriate sub-packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clear the session.  As in the [simple neural network example](./synthetic.ipynb), this step is not necessary, but it is good practice as it clears any model layers that you have built before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a very simple network.  The features are:\n",
    "*  We have one hidden layer with `nh=100` units.  \n",
    "*  One output layer with `nout=10` units, one for each of the 10 possible classes\n",
    "*  The output activation is `softmax`, which is used for multi-class targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nin = X.shape[1]  # dimension of input data\n",
    "nh = 100     # number of hidden units\n",
    "nout = int(np.max(y)+1)    # number of outputs = 10 since there are 10 classes\n",
    "model = Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(nout, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "As before, to train the network, we have to select an optimizer and a loss function.  Since this is a multi-class classification problem, we select the `sparse_categorial_crossentropy` loss.  We use the `adam` optimizer as before.  You may want to play with the learning rate `lr`.   We also set the `metrics` that we wish to track during the optimization.  In this case, we select `accuracy` on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a simple method `fit` to run the optimization.  You simply specify the number of epochs and the batch size, both discussed in class.  In addition, we specify the `validation_data` so that it can print the accuracy on the test data set as it performs the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.5532 - acc: 0.8594 - val_loss: 0.3216 - val_acc: 0.9113\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.2769 - acc: 0.9205 - val_loss: 0.2459 - val_acc: 0.9275\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.2202 - acc: 0.9368 - val_loss: 0.2090 - val_acc: 0.9396\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.1842 - acc: 0.9471 - val_loss: 0.1785 - val_acc: 0.9494\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.1574 - acc: 0.9551 - val_loss: 0.1608 - val_acc: 0.9534\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.1362 - acc: 0.9612 - val_loss: 0.1392 - val_acc: 0.9594\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.1198 - acc: 0.9661 - val_loss: 0.1305 - val_acc: 0.9621\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.1067 - acc: 0.9700 - val_loss: 0.1277 - val_acc: 0.9612\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.0959 - acc: 0.9735 - val_loss: 0.1163 - val_acc: 0.9653\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0873 - acc: 0.9752 - val_loss: 0.1132 - val_acc: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27525ec92e8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtr, ytr, epochs=10, batch_size=100, validation_data=(Xts,yts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the 10 epochs, you should obtain a test accuracy of around 96.5%.  If we run it for another a few epochs, we can get slightly higher results.  We can just run the `model.fit` command again, and it will start where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0789 - acc: 0.9782 - val_loss: 0.1077 - val_acc: 0.9687\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0724 - acc: 0.9804 - val_loss: 0.1069 - val_acc: 0.9683\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0663 - acc: 0.9822 - val_loss: 0.0963 - val_acc: 0.9713\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0604 - acc: 0.9837 - val_loss: 0.0995 - val_acc: 0.9699\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0538 - acc: 0.9861 - val_loss: 0.0968 - val_acc: 0.9693\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0520 - acc: 0.9858 - val_loss: 0.0921 - val_acc: 0.9714\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0474 - acc: 0.9868 - val_loss: 0.0886 - val_acc: 0.9717\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0440 - acc: 0.9884 - val_loss: 0.0875 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 2s - loss: 0.0393 - acc: 0.9903 - val_loss: 0.0872 - val_acc: 0.9732\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s - loss: 0.0381 - acc: 0.9901 - val_loss: 0.0875 - val_acc: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27525fff588>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtr, ytr, epochs=10, batch_size=100, validation_data=(Xts,yts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a little more than 97% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the model\n",
    "\n",
    "Since the training takes a long time, it is useful to save the results.  See the [keras page](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) for many more useful saving and loading features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"mnist_mod.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reload the model with the `load_model` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"mnist_mod.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the performance on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.971800\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(Xts, yts, verbose=0)\n",
    "print(\"accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
